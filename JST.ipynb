{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d172095-15b6-4c8d-ba0a-df31b9bd7355",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install jointtsmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05d30098-3609-4673-9140-de6f3524dd24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\User\\\\jh2\\\\JST'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62553c1-00c6-452f-afdd-60b4791df56b",
   "metadata": {},
   "source": [
    "### 1. 감성사전 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "255a6bd4-fbc9-414b-8f40-64720b6764e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\jh2\\lib\\site-packages\\pandas\\util\\_decorators.py:211: FutureWarning: the 'encoding' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'encoding'\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# KNU 감성사전 기반 lexicon 생성 (긍정단어 - 1, 2점 -> 1점 // 부정단어 - -1, -2점 -> -1점 // 중립단어 제외)\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "sentiment_df = pd.DataFrame(index=['Word','Sentiment'])\n",
    "word_list = []\n",
    "sentiment_list = []\n",
    "\n",
    "with open('SentiWord_info.json', encoding='utf-8') as f:\n",
    "    \n",
    "    data = json.load(f)\n",
    "    \n",
    "    for i in data:\n",
    "        root = i['word_root']\n",
    "        score = int(i['polarity'])\n",
    "        \n",
    "        if score >= 1:\n",
    "            score = 1\n",
    "        \n",
    "        elif score <= -1:\n",
    "            score = -1\n",
    "        \n",
    "        elif score == 0:\n",
    "            continue\n",
    "        \n",
    "        out = pd.DataFrame({'Word':[root], 'Sentiment':[score]})\n",
    "        sentiment_df = pd.concat([sentiment_df, out])\n",
    "    \n",
    "    \n",
    "sentiment_df = sentiment_df.reset_index(drop=True).dropna()\n",
    "sentiment_df.to_excel('prior_sentiment_kor.xlsx', encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69481d52-e52a-4763-b4d8-419e8bc4adb3",
   "metadata": {},
   "source": [
    "### 2.전처리함수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "fcbebbab-c3ca-42c8-9800-9e73017f81e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "#visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pylab as plb\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import seaborn as sns\n",
    "\n",
    "from konlpy.tag import Mecab, Kkma, Okt\n",
    "from iteration_utilities import unique_everseen\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib import font_manager, rc\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "path_list = ['C:/Users/User/Desktop/ESG_자료/esg_텍스트/분석용/전처리 이후/블로그/2020_2021/',\n",
    "             'C:/Users/User/Desktop/ESG_자료/esg_텍스트/분석용/전처리 이후/유튜브/2020_2021/',\n",
    "             'C:/Users/User/Desktop/ESG_자료/esg_텍스트/분석용/전처리 이후/인스타/2020_2021/',\n",
    "             'C:/Users/User/Desktop/ESG_자료/esg_텍스트/분석용/전처리 이후/트위터/2020_2021/',\n",
    "             'C:/Users/User/Desktop/ESG_자료/esg_텍스트/분석용/전처리 이후/블로그/2022',\n",
    "             'C:/Users/User/Desktop/ESG_자료/esg_텍스트/분석용/전처리 이후/유튜브/2022/',\n",
    "             'C:/Users/User/Desktop/ESG_자료/esg_텍스트/분석용/전처리 이후/인스타/2022',\n",
    "             'C:/Users/User/Desktop/ESG_자료/esg_텍스트/분석용/전처리 이후/트위터/2022/',\n",
    "             'C:/Users/User/Desktop/ESG_자료/esg_텍스트/분석용/보고서/']\n",
    "\n",
    "\n",
    "def file_process(n, dt='xlsx'):\n",
    "    # 경로설정\n",
    "    path = path_list[n]\n",
    "\n",
    "    # 결과 추출을 위한 이름 설정\n",
    "    name = set_name(path)\n",
    "\n",
    "    df = open_file(path, datatype=dt)\n",
    "\n",
    "    tqdm.pandas(desc=\"Tokenization Progress\")\n",
    "    df['tokens'] = df['text'].progress_apply(preprocess, custom_dict=custom_dict)\n",
    "    df['text'] = df['text'].progress_apply(pre_process)\n",
    "\n",
    "\n",
    "    # 중복되는 df 제거\n",
    "    df.drop_duplicates(subset=['text'], keep='first', inplace=True)\n",
    "    df['num_tokens'] = df['tokens'].map(len)\n",
    "\n",
    "    # 토큰값이 0인 행 제거\n",
    "    df = df.loc[df['num_tokens'] != 0]\n",
    "\n",
    "    # index 초기화\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "파일 불러오기를 위한 함수 설정 \n",
    "\n",
    "- path: 경로\n",
    "- datatype= 'docx' or 'xlsx'\n",
    "- min_text_docx = n 문단최소글자수 (default == 5)\n",
    "- min_text_xlsx = n 엑셀 최소 글자수 (default == 5)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def open_file(path, datatype=None, min_text_docx = 5, min_text_xlsx = 5):\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    files_xlsx = []\n",
    "    files_docx = []\n",
    "\n",
    "    if datatype == 'docx':\n",
    "        files_docx = [file for file in os.listdir(path) if file.endswith('.docx')]\n",
    "        paragraphs = []\n",
    "        titles = []\n",
    "        \n",
    "        for i in range(len(files_docx)):\n",
    "            file_name = path+files_docx[i]\n",
    "            text = docx2txt.process(file_name)\n",
    "            doc_paragraphs = [p.strip() for p in text.split('\\n'*3) if len(p.strip()) > min_text_docx]\n",
    "            # doc_paragraphs = [p.strip() for p in re.split('\\n+', text) if len(p.strip()) > min_text_docx]            \n",
    "            \n",
    "            titles += [files_docx[i]] * len(doc_paragraphs)\n",
    "            paragraphs += doc_paragraphs\n",
    "            \n",
    "        df = pd.DataFrame(paragraphs, columns=['text'])\n",
    "        # df = pd.DataFrame(list(zip(titles, paragraphs)), columns=['title', 'text'])\n",
    "\n",
    "    elif datatype == 'xlsx':\n",
    "        files_xlsx = [file for file in os.listdir(path) if file.endswith('.xlsx')]\n",
    "        df = pd.DataFrame()\n",
    "        \n",
    "        if '유튜브' in path:\n",
    "            for f in files_xlsx:\n",
    "                file_path = os.path.join(path, f)\n",
    "                \n",
    "                df_title = pd.read_excel(file_path, usecols=['제목'])\n",
    "                df_title = df_title.rename(columns={'제목': 'text'})   \n",
    "                df_title = df_title.drop_duplicates(keep='first')\n",
    "\n",
    "                df_text = pd.read_excel(file_path, usecols=['댓글'])\n",
    "                df_text = df_text.rename(columns={'댓글': 'text'})   \n",
    "                \n",
    "                df = pd.concat([df, df_title, df_text], ignore_index=True)\n",
    "                \n",
    "#         elif '인스타' in path:\n",
    "#             for f in files_xlsx:\n",
    "#                 file_path = os.path.join(path, f)\n",
    "                \n",
    "#                 df_text = pd.read_excel(file_path, usecols=['본문'])\n",
    "#                 df_text = df_text.rename(columns={'본문': 'text'})   \n",
    "#                 df_text = df_text.drop_duplicates(keep='first')\n",
    "\n",
    "#                 df_com = pd.read_excel(file_path, usecols=['댓글'])\n",
    "#                 df_com = df_com.rename(columns={'댓글': 'text'})  \n",
    "                \n",
    "                \n",
    "#                 df = pd.concat([df, df_text, df_com], ignore_index=True)\n",
    "\n",
    "        # 댓글 각자 문서로 처리\n",
    "    \n",
    "        elif '인스타' in path:\n",
    "            for f in files_xlsx:\n",
    "                file_path = os.path.join(path, f)\n",
    "                df_text = pd.read_excel(file_path, usecols=['본문'])\n",
    "                df_text = df_text.rename(columns={'본문': 'text'})\n",
    "                df_text = df_text.drop_duplicates(keep='first')\n",
    "                df_com = pd.read_excel(file_path, usecols=['댓글'])\n",
    "                df_com = df_com.rename(columns={'댓글': 'text'})\n",
    "                \n",
    "                # 각 댓글을 하나의 문서로 분리\n",
    "                df_com['text'] = df_com['text'].str.split('/')\n",
    "                df_com = df_com.explode('text')\n",
    "                \n",
    "                df = pd.concat([df, df_text, df_com], ignore_index=True)\n",
    "\n",
    "        elif '트위터' in path:\n",
    "            for f in files_xlsx:\n",
    "                file_path = os.path.join(path, f)\n",
    "                df_read = pd.read_excel(file_path, usecols=['text'])\n",
    "                df = pd.concat([df, df_read])\n",
    "\n",
    "        elif '블로그' in path:\n",
    "            \n",
    "            for f in files_xlsx:\n",
    "                file_path = os.path.join(path, f)\n",
    "                \n",
    "                # df_title = pd.read_excel(file_path, usecols=['title'])\n",
    "                # df_title = df_title.rename(columns={'title': 'text'})   \n",
    "                # df_title = df_title.drop_duplicates(keep='first')\n",
    "\n",
    "                df_text = pd.read_excel(file_path, usecols=['text'])\n",
    "                df = pd.concat([df, df_text], ignore_index=True)\n",
    "\n",
    "                # df = pd.concat([df, df_title, df_text], ignore_index=True)\n",
    "                \n",
    "\n",
    "    return df.dropna()\n",
    "\n",
    "# 형태소 분석을 위한 객체 생성\n",
    "mecab = Mecab(dicpath='c:/mecab/mecab-ko-dic')\n",
    "\n",
    "# 1차 전처리 (한글 이외에 모든 텍스트 제거)\n",
    "def pre_process(text):\n",
    "    \n",
    "    pr_text = re.sub(r\"[^ㄱ-ㅣ가-힣\\s]+|[ㄱ-ㅎㅏ-ㅣ]+\", \"\", text)\n",
    "    pr_text = re.sub('\\\\<br\\\\>',' ',pr_text)\n",
    "    pr_text = re.sub(r'\\s+', ' ', pr_text, flags=re.I)\n",
    "    pr_text = pr_text.replace(\"\\n\", \"\")\n",
    "    pr_text = pr_text.strip()\n",
    "    return pr_text\n",
    "\n",
    "# 2차 전처리 mecab을 통한 형태소 분석\n",
    "def analyzer_pos(text, custom_dict):\n",
    "    tokens = mecab.pos(text)\n",
    "    # tokens = [ s for s, t in tokens if t in ['XR', 'VV', 'VA', 'NNG', 'NNP'] and len(s) > 1 and s not in stop_word] \n",
    "    # tokens = [ s for s, t in tokens if len(s) > 1 and s not in stop_word] # 전체 품사 중 3글자 이상 + 불용어를 제외함\n",
    "    tokens = [ s for s, t in tokens if t in ['VV', 'VA', 'NNG', 'NNP'] and len(s) > 1 and s not in stop_word] # 동사, 형용사, 명사 중 2글자 이상 + 불용어를 제외함\n",
    "\n",
    "    # 원하는 단어가 끊여져서 나올경우(우선순위 설정 이후) 합치기\n",
    "    i = 0\n",
    "    new_tokens = []\n",
    "\n",
    "    while i < len(tokens):\n",
    "        if i == len(tokens) - 1:\n",
    "            new_tokens.append(tokens[i])\n",
    "            break\n",
    "        current_token = tokens[i]\n",
    "        next_token = tokens[i+1]\n",
    "        if current_token+next_token in custom_dict:\n",
    "            new_tokens.append(current_token+next_token)         \n",
    "            i += 2\n",
    "        else:\n",
    "            new_tokens.append(current_token)\n",
    "            i += 1\n",
    "    return new_tokens\n",
    "\n",
    "\n",
    "def set_name(path):\n",
    "    name = ''\n",
    "    \n",
    "    if '보고서' in path:\n",
    "        name = '보고서'\n",
    "\n",
    "    elif '인스타' in path:\n",
    "        name = '인스타'\n",
    "\n",
    "    elif '블로그' in path:\n",
    "        name = '블로그'\n",
    "\n",
    "    elif '트위터' in path:\n",
    "        name = '트위터'\n",
    "\n",
    "    elif '유튜브' in path:\n",
    "        name = '유튜브'\n",
    "    \n",
    "    return name\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "텍스트를 입력받아 사용자 정의 사전을 기반으로 정규표현식 기반 전처리, mecab 형태소 분석을 진행\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def preprocess(text, custom_dict=None):\n",
    "    \n",
    "    # 정규표현식 처리\n",
    "    text = pre_process(text)\n",
    "    \n",
    "    # mecab 전처리\n",
    "    tokens = analyzer_pos(text, custom_dict)\n",
    "    # tokens = \", \".join(tokens)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6027ed25-4c69-41d2-a678-359afa05a9a3",
   "metadata": {},
   "source": [
    "### 3. 불용어사전 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e708fe24-0ef2-4f27-982f-afd4d5c98d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['가량', '가지', '각', '간', '갖은', '개', '개국', '개년', '개소', '개월', '걔', '거', '거기', '거리', '건', '것', '겨를', '격,', '겸', '고', '군', '군데', '권', '그', '그거', '그것', '그곳', '그까짓', '그네', '그녀', '그놈', '그대', '그래', '그래도,', '그서', '그러나', '그러니', '그러니까', '그러다가', '그러면', '그러면서', '그러므로', '그러자', '그런', '그런', '데', '그럼', '그렇지만', '그루', '그리고', '그리하여', '그분', '그이', '그쪽', '근', '근데', '글쎄', '글쎄요', '기,', '김', '나', '나름', '나위', '남짓', '내', '냥', '너', '너희', '네', '네놈', '녀석', '년', '년대', '년도', '놈', '누구', '니,', '다른', '다만', '단', '달', '달러', '당신', '대', '대로', '더구나', '더욱이', '데', '도', '동', '되', '두', '두세', '두어,', '둥', '듯', '듯이', '등', '등등', '등지', '따라서', '따름', '따위', '딴', '때문', '또', '또는', '또한', '리', '마당', '마련,', '마리', '만', '만큼', '말', '매', '맨', '명', '몇', '몇몇', '모', '모금', '모든', '무렵', '무슨', '무엇', '뭐', '뭣', '미터,', '및', '바', '바람', '바퀴', '박', '발', '발짝', '번', '벌', '법', '별', '본', '부', '분', '뻔', '뿐', '살', '새', '서너', '석', '설,', '섬', '세', '세기', '셈', '쇤네', '수', '순', '스무', '승', '시', '시간', '식', '씨', '아', '아냐', '아니', '아니야', '아무,', '아무개', '아무런', '아아', '아이', '아이고', '아이구', '야', '약', '양', '얘', '어', '어느', '어디', '어머', '언제', '에이,', '엔', '여기', '여느', '여러', '여러분', '여보', '여보세요', '여지', '역시', '예', '옛', '오', '오랜', '오히려', '온', '온갖,', '올', '왜냐하면', '왠', '외', '요', '우리', '원', '월', '웬', '위', '음', '응', '이', '이거', '이것', '이곳', '이놈', '이래,', '이런', '이런저런', '이른바', '이리하여', '이쪽', '일', '일대', '임마', '자', '자기', '자네', '장', '저', '저것', '저기,', '저놈', '저런', '저쪽', '저편', '저희', '적', '전', '점', '제', '조', '주', '주년', '주일', '줄', '중', '즈음', '즉', '지', '지경,', '지난', '집', '짝', '쪽', '쯤', '차', '참', '채', '척', '첫', '체', '초', '총', '측', '치', '큰', '킬로미터', '타', '터', '턱', '톤,', '통', '투', '판', '퍼센트', '편', '평', '푼', '하기야', '하긴', '하물며', '하지만', '한', '한두', '한편', '허허', '헌', '현,', '호', '혹은', '회', '흥', '최태원', '디스클로저', '외신', '출처', '한국', '뉴스', '최초', '공개', '단독', '회사', '상장', '보도', '글로벌', '발행', '래퍼', '출연', '리뷰', '중앙일보', '기자', '이낙연', '디렉', '명상', '회장', '신문', '나라', '블로그', '코리아', '미국', '중국', '관련', '대한민국', '대리석', '타임스', '무디스', '법무법인', '변호사', '교수', '기업', '타임즈', '인더스', '트리', '습니다', '에서', '속도', '한다', '이승기', '방위대', '일본', '는데', '으로', '처럼', '호남', '야구', '위한', '면서', '지만', '에게', '천억', '투데이', '해요', '펭귄', '팝콘', '북마크', '네이버', '네슬레', '자칭', '네요', '코엑스', '노스페이스', '홈페이지', '어야', '전라', '전북', '매일신문', '감사', '영상', '만들', '방송', '슈카', '오늘', '배우', '세상', '사람', '다니', '애플', '나오', '삼전', '민주당', '선생', '선수', '이번', '마찬가지', '그동안', '동원', '참치', '맛있', '한화', '그룹', '문재인', '래원', '노래', '좌파', '휠라', '기판', '하이닉스', '한미', '입니다', '게임', '넷마블', '삼성', '시멘트', '조선', '백화점', '약품', '의약품', '해양', '효성', '렌탈', '울산', '자동차', '그램', '정답', '빅이슈', '삼양', '두부', '부면', '포스코', '하림', '비빔면', '상공', '도시락', '스타', '미주', '라무', '유한', '킴벌리', '농협', '모델', '프로', '아거', '연지', '김병만', '떨채', '리비', '프로', '퀴즈', '경기주택공사', '경기', '주택', '공사', '진분', '테슬라', '교보생명', '푸르', '서울', '공단', '삼성전자', '부문', '카카오', '데일리', '임팩트', '국내', '시사', '센터', '앨범', '지디', '엔터', '부산', '연구원', '이벤트', '출장', '박스', '안치용', '슈가버블', '배민', '산불', '피해', '클릭', '지수', '하이브', '호비', '경기도', '래리', '핑크', '기사', '링크', '윤석열', '생각', '케어', '오전', '오후', '제주', '가능', '내용', '경우', '정도', '소개', '대표', '언서', '학교', '대학교', '학생', '사진', '커피', '피부', '생일', '탄생', '월드', '친구', '모닝', '생일']\n",
      "505\n"
     ]
    }
   ],
   "source": [
    "# 불용어\n",
    "# 길호현. (2018). 텍스트마이닝을 위한 한국어 불용어 목록 연구. 우리말글, 78, 1-25.\n",
    "\n",
    "with open(\"stop.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    stop_word = []\n",
    "    stop_text = [stop_word.extend(line.strip().split(\", \")) for line in f]\n",
    "\n",
    "\n",
    "# 개인 전처리를 위한 stop_word 설정\n",
    "stop_word_jh = ['최태원', '디스클로저', '외신', '출처', '한국', '뉴스', '최초', '공개', '단독', '회사', '상장', '보도', '글로벌', '발행', '래퍼', '출연', '리뷰', '중앙일보', '기자', '이낙연',\n",
    "                '디렉', '명상', '회장', '신문', '나라', '블로그', '코리아', '미국', '중국', '관련', '대한민국', '대리석', '타임스', '무디스', '법무법인', '변호사', '교수', '기업',\n",
    "                '타임즈', '인더스', '트리', '습니다', '에서', '속도', '한다', '이승기', '방위대', '일본', '는데', '으로', '처럼', '호남', '야구', '위한', '면서', '지만', '에게', '천억', '투데이', '해요', '펭귄', '팝콘',\n",
    "                '북마크', '네이버', '네슬레', '자칭', '네요', '코엑스', '노스페이스', '홈페이지', '어야', '전라', '전북', '매일신문', '감사', '영상', '만들', '방송', '슈카', '오늘', '배우', '세상', '사람',\n",
    "                '다니', '애플', '나오', '삼전', '민주당', '선생', '선수', '이번', '마찬가지', '그동안', '동원', '참치', '맛있', '한화', '그룹', '문재인', '래원', '노래', '좌파', '휠라', '기판',\n",
    "                '하이닉스', '한미', '입니다', '게임', '넷마블', '삼성', '시멘트', '조선', '백화점', '약품', '의약품', '해양', '효성', '렌탈', '울산', '자동차', '그램', '정답',\n",
    "                '빅이슈', '삼양', '두부', '부면', '포스코', '하림', '비빔면', '상공', '도시락', '스타', '미주', '라무', '유한', '킴벌리', '농협', '모델', '프로', '아거', '연지', '김병만',\n",
    "                '떨채', '리비', '프로', '퀴즈', '경기주택공사', '경기', '주택', '공사', '진분', '테슬라', '교보생명', '푸르', '서울', '공단', '삼성전자', '부문', '카카오', '데일리', '임팩트',\n",
    "                '국내', '시사', '센터', '앨범', '지디', '엔터', '부산', '연구원', '이벤트', '출장', '박스', '안치용', '슈가버블', '배민', '산불', '피해', '클릭', '지수', '하이브', '호비', '경기도',\n",
    "                '래리', '핑크', '기사', '링크', '윤석열', '생각', '케어', '오전', '오후', '제주', '가능', '내용', '경우', '정도', '소개', '대표', '언서', '학교', '대학교', '학생', '사진', '커피',\n",
    "                '피부', '생일', '탄생', '월드', '친구', '모닝', '생일']\n",
    "\n",
    "stop_word.extend(stop_word_jh)\n",
    "print(stop_word)\n",
    "print(len(stop_word))\n",
    "\n",
    "custom_dict = ['지속가능', '이해관계자', '탄소중립', '신재생에너지', '재생에너지', '탄소중립', '탄소제로', '메타버스', '지배구조', '지속가능보고서', '지속가능경영보고서', '사외이사',\n",
    "               '이상기후', '기후변화', '게임체인저', '동반성장', '고부가가치', '협력사', '가치사슬', '밸류체인', '유연근무', '정보보호', '이행원칙', '노사협의회', '위드코로나', '제로웨이스트']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea84702-f9ff-41f6-8682-bf16204652ab",
   "metadata": {},
   "source": [
    "### 4. 데이터 호출 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "96b6315c-d5b3-43b4-af27-8424f542c1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization Progress: 100%|████████████████████████████████████████████████████| 31899/31899 [01:31<00:00, 348.76it/s]\n",
      "Tokenization Progress: 100%|███████████████████████████████████████████████████| 31899/31899 [00:03<00:00, 8077.11it/s]\n",
      "Tokenization Progress: 100%|█████████████████████████████████████████████████████| 8889/8889 [00:01<00:00, 8197.93it/s]\n",
      "Tokenization Progress: 100%|███████████████████████████████████████████████████| 8889/8889 [00:00<00:00, 110099.04it/s]\n",
      "Tokenization Progress: 100%|███████████████████████████████████████████████████| 23339/23339 [00:04<00:00, 5198.44it/s]\n",
      "Tokenization Progress: 100%|██████████████████████████████████████████████████| 23339/23339 [00:00<00:00, 84790.77it/s]\n",
      "Tokenization Progress: 100%|█████████████████████████████████████████████████████| 4357/4357 [00:00<00:00, 6767.13it/s]\n",
      "Tokenization Progress: 100%|████████████████████████████████████████████████████| 4357/4357 [00:00<00:00, 99356.18it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import docx2txt\n",
    "from tqdm import tqdm \n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "path_list (블로그/유튜브/인스타/트위터) + 보고서\n",
    "[0~3] - 2020/2021  0:블로그 / 1: 유튜브 / 2: 인스타 / 3: 트위터\n",
    "[4~7] - 2022       4:블로그 / 5: 유튜브 / 6: 인스타 / 7: 트위터\n",
    "[8]   - 보고서      8: 보고서\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "name = set_name(path)\n",
    "df = pd.DataFrame()\n",
    "# df = file_process(4)\n",
    "\n",
    "df_blog = file_process(4)\n",
    "df_yt = file_process(5)\n",
    "df_insta = file_process(6)\n",
    "df_twitter = file_process(7)\n",
    "\n",
    "df = pd.concat([df, df_blog, df_yt, df_insta, df_twitter], ignore_index=True)\n",
    "# df.to_excel('소셜미디어_전처리.xlsx', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "357a225f-3b75-42d7-9e5c-60d0a6f346c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('소셜미디어_전처리.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfeb1eb-96c6-47cc-8a43-edff4cfb60e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c021172-1136-492e-acde-51114684e08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어사전 확인\n",
    "with open(\"C:\\\\mecab\\\\user-dic\\\\nnp.csv\", 'r', encoding='utf-8') as f: \n",
    "    file_data = f.readlines()\n",
    "file_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf91672-2e34-4d24-98a6-88469ad6d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 우선순위 확인\n",
    "with open(\"c:/mecab/mecab-ko-dic/user-nnp.csv\", 'r', encoding='utf-8') as f: \n",
    "    file_data = f.readlines()\n",
    "file_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "dc5ff125-c381-44bb-af5a-64ca355be000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997cc4f2-aa34-486f-8b4c-5a529515b47f",
   "metadata": {},
   "source": [
    "### 5.JST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "cb2e1f84-93cb-4658-bd17-3b7a28bf89c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jointtsmodel.RJST import RJST\n",
    "from jointtsmodel.JST import JST\n",
    "from jointtsmodel.sLDA import sLDA\n",
    "from jointtsmodel.TSM import TSM\n",
    "from jointtsmodel.TSWE import TSWE\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from jointtsmodel.utils import *\n",
    "\n",
    "\n",
    "text = df['tokens'].apply(lambda x: ' '.join(x))\n",
    "vectorizer = CountVectorizer(max_df=0.5,\n",
    "                             min_df=5,\n",
    "                             ngram_range=(1,2),          #바이그램 사용시 성능 확인\n",
    "                             \n",
    "                             max_features=3000)\n",
    "\n",
    "X = vectorizer.fit_transform(text)\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "inv_vocabulary = dict(zip(vocabulary,np.arange(len(vocabulary))))\n",
    "lexicon_data = pd.read_excel('./prior_sentiment_kor.xlsx')\n",
    "lexicon_data = lexicon_data.dropna()\n",
    "lexicon_dict = dict(zip(lexicon_data['Word'],lexicon_data['Sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b853da7a-88e5-42d9-afcb-920b592c457b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['가게', '가격', '가격 상승', '가계', '가공', '가구', '가동', '가뭄', '가방', '가볍',\n",
       "       '가상', '가속', '가수', '가스', '가슴', '가시', '가요', '가운데', '가을', '가이드',\n",
       "       '가이드라인', '가입', '가전', '가정', '가져가', '가족', '가죽', '가치', '가치 가치',\n",
       "       '가치 동시', '가치 사회', '가치 소비', '가치 실현', '가치 창출', '가치 평가', '가치관', '각각',\n",
       "       '각국', '각자', '각종', '간담회', '간식', '간접', '갈등', '감각', '감독', '감동', '감성',\n",
       "       '감소', '감시', '감안', '감정', '감축', '감축 목표', '강남', '강남구', '강릉', '강사',\n",
       "       '강산', '강세', '강아지', '강연', '강원', '강원도', '강의', '강점', '강제', '강조', '강하',\n",
       "       '강화', '갖추', '개강', '개념', '개미', '개발', '개발 사업', '개발 협력', '개발자', '개방',\n",
       "       '개별', '개사', '개선', '개설', '개요', '개인', '개인 정보', '개입', '개정', '개척',\n",
       "       '개최', '개편', '개혁', '갤러리', '갤럭시', '거대', '거래', '거래소', '거버넌스', '거부',\n",
       "       '거절'], dtype=object)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3fbd5b86-5bf8-4c86-af6d-93aa941f6f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity after iteration 10 (out of 10 iterations) is 3668.68\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1.0008362915306384"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# for n in range(3, 16):\n",
    "\n",
    "model = JST(n_topic_components=3, n_sentiment_components=3,random_state=123,evaluate_every=10)\n",
    "model.fit(X.toarray(), lexicon_dict)\n",
    "\n",
    "# model.transform()[:2]\n",
    "\n",
    "top_words = list(model.getTopKWords(vocabulary).values())\n",
    "\n",
    "'''\n",
    "utils.py // def coherence_score_umass() 깃허브 확인 후 코드 수정 (127~129) 2023-03-19\n",
    "\n",
    "'''\n",
    "coherence_score_umass(X.toarray(),inv_vocabulary,top_words)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a9ae8bfc-8112-45d2-9ea9-150a3504ff3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 1): ['활동', '사회', '환경', '진행', '경영', '친환경', '참여', '활용', '실천', '제품'],\n",
       " (1, 2): ['사회', '경영', '활동', '교육', '지원', '환경', '진행', '가치', '참여', '사업'],\n",
       " (1, 3): ['친환경', '환경', '제품', '경영', '사용', '활용', '브랜드', '사업', '사회', '기술'],\n",
       " (2, 1): ['면접', '사업', '지원', '경영', '산업', '기술', '투자', '에너지', '시장', '안전'],\n",
       " (2, 2): ['사용', '환경', '시작', '제품', '친환경', '세대', '경영', '사랑', '브랜드', '선물'],\n",
       " (2, 3): ['투자', '경제', '시장', '가격', '세계', '에너지', '상승', '산업', '경영', '환경'],\n",
       " (3, 1): ['환경', '경영', '사회', '사용', '제품', '친환경', '실천', '인증', '지구', '여행'],\n",
       " (3, 2): ['경영', '환경', '사회', '면접', '평가', '탄소', '배출', '지원', '사업', '금융'],\n",
       " (3, 3): ['사회', '가치', '경제', '경영', '환경', '기술', '디지털', '투자', '챌린지', '금융']}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.getTopKWords(vocabulary, num_words=10)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa9f199-faca-43a1-af84-992060ac51a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수정사항 확인용\n",
    "import inspect\n",
    "print(inspect.getsource(coherence_score_umass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b0dbc21-7b0c-4600-9772-3fe0ad6d2b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1651.2570725141406"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.perplexity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "25d467da-8af7-497e-a97b-74c31c0c81a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = list(model.getTopKWords(vocabulary).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b3c4d9-0157-44bc-9a12-928c2798944a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
